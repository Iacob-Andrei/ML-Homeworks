{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Machine Learning, Fall 2022\n",
    "\n",
    "Deadline: 11th of December 2022, 23:59\n",
    "\n",
    "To do this project you have to complete this Jupyter notebook and send it via Discord.\n",
    "\n",
    "The total number of points allocated for this project is 10.\n",
    "\n",
    "You will need the following modules to solve the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from itertools import product\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sex  island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0    0       2            39.1           18.7              181.0       3750.0   \n",
      "1    1       2            39.5           17.4              186.0       3800.0   \n",
      "2    1       2            40.3           18.0              195.0       3250.0   \n",
      "3    1       2            36.7           19.3              193.0       3450.0   \n",
      "4    0       2            39.3           20.6              190.0       3650.0   \n",
      "\n",
      "   species  \n",
      "0        1  \n",
      "1        1  \n",
      "2        1  \n",
      "3        1  \n",
      "4        1  \n",
      "   sex  island  species\n",
      "0    0       2        1\n",
      "1    1       2        1\n",
      "2    1       2        1\n",
      "3    1       2        1\n",
      "4    0       2        1\n"
     ]
    }
   ],
   "source": [
    "penguin_dataset = pd.read_csv(\"data/penguins_filtered.csv\")\n",
    "\n",
    "penguin_dataset = penguin_dataset.replace({\n",
    "    \"Adelie\": 1,\n",
    "    \"Chinstrap\" : 2,\n",
    "    \"Gentoo\": 3,\n",
    "    \"male\" : 0,\n",
    "    \"female\" : 1,\n",
    "    \"Biscoe\" : 0,\n",
    "    \"Dream\" : 1,\n",
    "    \"Torgersen\" : 2})\n",
    "\n",
    "discrete_penguin_dataset = penguin_dataset[[\"sex\", \"island\", \"species\"]]\n",
    "print(penguin_dataset.head())\n",
    "print(discrete_penguin_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Naive and Joint Bayes (3.5 points; 0.15 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the prior probabilities for the target feature. Transform them by applying the natural logarithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.8245358682721073, -1.5886347848043372, -1.0290189968689145]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def prior_probability_target(dataset, target):\n",
    "    prob = [ list(dataset[target]).count(val) / len(dataset[target]) for val in np.unique(dataset[target])]\n",
    "    prob = list(map(lambda x: math.log(x), prob))\n",
    "    return prob\n",
    "\n",
    "prior_probability_target(penguin_dataset, 'species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write the formulas used to calculate the maximum aposteriori probability using Naive Bayes and Joint Bayes. Use the names of the variables from the discrete penguin dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer here*:\n",
    "\n",
    "**Naive Bayes:** \n",
    "* s = argmax P(sex|species = s) * P(island|species = s) * P(species = s)\n",
    "\n",
    "**Joint Bayes:** \n",
    "* s = argmax P(sex, island | species = s) * P(species = s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find and calculate the logarithm of all the conditional probabilities (also names likelihoods) used to predict the label for the instance `{\"sex\" : 1, \"island\" : 2}` in Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiaco\\AppData\\Local\\Temp\\ipykernel_14764\\3861210417.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  probabilities.append([np.log(sex_count/species_count), np.log(island_count/species_count)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-0.6931471805599453, -1.133459019998278],\n",
       " [-0.6931471805599453, -inf],\n",
       " [-0.7186804825651101, -inf]]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def likelihoods(dataset, target, sex_value, island_value):\n",
    "    probabilities = list()\n",
    "\n",
    "    for species_value in np.unique(dataset[target]):\n",
    "        ds = dataset[dataset[target] == species_value].reset_index(drop=True)\n",
    "\n",
    "        sex_count = list(ds['sex']).count(sex_value)\n",
    "        island_count = list(ds['island']).count(island_value)\n",
    "        species_count = list(dataset[target]).count(species_value)\n",
    "\n",
    "        probabilities.append([np.log(sex_count/species_count), np.log(island_count/species_count)])\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "likelihoods(discrete_penguin_dataset, 'species', 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Why does the results contain infinity? Fix the calculation by using the Laplace Smoothing with `alpha = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.6931471805599453, -1.1327452950375683],\n",
       " [-0.6931471805599453, -4.2626798770413155],\n",
       " [-0.7182531016910216, -4.804021044733257]]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def likelihoods_laplace(dataset, target, sex_value, island_value, alpha = 1):\n",
    "    probabilities = list()\n",
    "\n",
    "    for species_value in np.unique(dataset[target]):\n",
    "        ds = dataset[dataset[target] == species_value].reset_index(drop=True)\n",
    "\n",
    "        sex_count = list(ds['sex']).count(sex_value) + alpha\n",
    "        island_count = list(ds['island']).count(island_value) + alpha\n",
    "        species_count = len(ds['species'])\n",
    "\n",
    "        probabilities.append ([math.log(sex_count    / (species_count + alpha*len(np.unique(dataset['sex'])))),\n",
    "                               math.log(island_count / (species_count + alpha*len(np.unique(dataset['island']))))] )\n",
    "    return probabilities\n",
    "\n",
    "likelihoods_laplace(discrete_penguin_dataset, 'species', 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Calculate the aposteriori probabilities of the labels and decide which label will Naive Bayes predict for the instance. Use only the logarithm values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.9609956286737821), (2, 0.019568798053176163), (3, 0.01943557327304175)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def aposteriori(dataset, target, sex_value, island_value):\n",
    "    priori = prior_probability_target(dataset, target)\n",
    "    likelihood = likelihoods_laplace(dataset, target, sex_value, island_value)\n",
    "    \n",
    "    prod = list()\n",
    "    for index in range(len(priori)):\n",
    "        prod.append( (index + 1, math.exp(sum(likelihood[index], priori[index])) ) )\n",
    "\n",
    "    suma = 0\n",
    "    for index in range(len(prod)):\n",
    "        suma += prod[index][1]\n",
    "\n",
    "    prod = list(map(lambda x: x[1]/suma, prod))\n",
    "    for index in range(3):\n",
    "        prod[index] = (index+1, prod[index])\n",
    "    return prod\n",
    "    \n",
    "aposteriori(discrete_penguin_dataset, 'species', 1 , 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Naive Bayes implemenation**: write a function called `naive_bayes` that takes three arguments:\n",
    "- `df`: the dataframe which will be used for training\n",
    "- `index_target`: the index of the column associated with the target feature\n",
    "- `alpha`: the parameter used for Laplace Smoothing\n",
    "\n",
    "The function should return a dictionary with the following fields:\n",
    "- `log_prior`: the logarithmic values of the prior probabilities (the probability of the labels)\n",
    "- `log_likelihoods`: a n x m x t array, where n - the number of features; m - the number of labels; t - the number of values for a feature; this array will contain the logarithmic values of the likelihoods (P(feature = value | target_feature = label))\n",
    "- `n_classes`: the number of labels\n",
    "- `n_feature_classes`: a vector that contains the number of unique values for each attribute\n",
    "- `classes`: the name of the labels (the values of the target feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': 3,\n",
      " 'log_likelihoods': {'island': {0: {1: -1.1972838161751393,\n",
      "                                    2: -4.2626798770413155,\n",
      "                                    3: -0.016529301951210582},\n",
      "                                1: {1: -0.9785946152103099,\n",
      "                                    2: -0.028573372444056,\n",
      "                                    3: -4.804021044733257},\n",
      "                                2: {1: -1.1327452950375683,\n",
      "                                    2: -4.2626798770413155,\n",
      "                                    3: -4.804021044733257}},\n",
      "                     'sex': {0: {1: -0.6931471805599453,\n",
      "                                 2: -0.6931471805599453,\n",
      "                                 3: -0.6686561605516496},\n",
      "                             1: {1: -0.6931471805599453,\n",
      "                                 2: -0.6931471805599453,\n",
      "                                 3: -0.7182531016910216}}},\n",
      " 'log_prior': [-0.8245358682721073, -1.5886347848043372, -1.0290189968689145],\n",
      " 'n_classes': [1, 2, 3],\n",
      " 'n_feature_classes': [[0, 1], [0, 1, 2]]}\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes(df, index_target = -1, alpha = 1):\n",
    "    target = df.columns[index_target]\n",
    "    output = dict()\n",
    "    output['log_prior'] = prior_probability_target(df, target)\n",
    "    output['n_classes'] = list(np.unique(df[target]))\n",
    "    output['n_feature_classes'] = [ list(np.unique(df[col])) for col in df.columns[:-1] ]\n",
    "    output['classes'] = len(np.unique(df[target]))\n",
    "    \n",
    "    sex_list = dict()\n",
    "    for sex in output['n_feature_classes'][0]:\n",
    "        prob = likelihoods_laplace(df, target, sex, 0, alpha)\n",
    "        sex_list[sex] = {1: prob[0][0], 2: prob[1][0], 3: prob[2][0]}\n",
    "\n",
    "    island_list = dict()\n",
    "    for island in output['n_feature_classes'][1]:\n",
    "        prob = likelihoods_laplace(df, target, 0, island, alpha)\n",
    "        island_list[island] = {1: prob[0][1], 2: prob[1][1], 3: prob[2][1]}\n",
    "\n",
    "    \n",
    "    output['log_likelihoods'] = {'sex': sex_list, 'island': island_list}\n",
    "    return output\n",
    "\n",
    "pprint(naive_bayes(discrete_penguin_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Train the discrete penguin dataset using your version of Naive Bayes and sklearn's. Compare the values of the parameters.(Be careful at what type of Naive Bayes classifier you pick from sklearn!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.69314718, -0.69314718],\n",
      "       [-0.69314718, -0.69314718],\n",
      "       [-0.66865616, -0.7182531 ]]), array([[-1.19728382, -0.97859462, -1.1327453 ],\n",
      "       [-4.26267988, -0.02857337, -4.26267988],\n",
      "       [-0.0165293 , -4.80402104, -4.80402104]])]\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "X = discrete_penguin_dataset[['sex','island']]\n",
    "y = discrete_penguin_dataset['species']\n",
    "cl = CategoricalNB(alpha=1).fit(X, y)\n",
    "\n",
    "new_messages = pd.DataFrame(\n",
    "  [(1, 2)],\n",
    "columns = ['sex','island'])\n",
    "\n",
    "#print(cl.class_log_prior_)\n",
    "print(cl.feature_log_prob_)\n",
    "# print(cl.predict(new_messages), '\\n', cl.predict_proba(new_messages))\n",
    "#print(cl.predict_proba(new_messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a function named `nb_predict_prob` that uses the log probabilities calculated by Naive Bayes to infer the aposteriori probability of a new instance `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.6504283438696206, -6.544461842405598, -6.551293143293193]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nb_predict_prob(nb_dict, X, use_log = False):\n",
    "    prob = list()\n",
    "    for trg in nb_dict['n_classes']:\n",
    "        prb = nb_dict['log_prior'][trg-1] + nb_dict['log_likelihoods']['sex'][X[0]][trg] + nb_dict['log_likelihoods']['island'][X[1]][trg]\n",
    "        prob.append(prb)\n",
    "\n",
    "    return prob\n",
    "\n",
    "nb_predict_prob(naive_bayes(discrete_penguin_dataset), [1,2], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Create a function that does the Naive Bayes prediction using the Maximum Aposteriori Probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nb_predict(nb_dict, X):\n",
    "    prob = nb_predict_prob(nb_dict, X)\n",
    "    return prob.index(max(prob)) + 1\n",
    "\n",
    "nb_predict(naive_bayes(discrete_penguin_dataset), [1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a function that calculate the accuracy of the trained model on a set of instances `X` with known labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_score(nb_dict, X, y):\n",
    "    ok = 0\n",
    "    for index in range(len(X)):\n",
    "        rez = nb_predict(nb_dict, list(X.loc[index]))\n",
    "        if rez == y.loc[index]:\n",
    "            ok += 1\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Calculate the training accuracy of your Naive Bayes algorithm. Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7027027027027027"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "X = discrete_penguin_dataset[list(discrete_penguin_dataset.columns[:-1])]\n",
    "y = discrete_penguin_dataset[discrete_penguin_dataset.columns[-1]]\n",
    "nb_score(naive_bayes(discrete_penguin_dataset), X, y)/len(discrete_penguin_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Find and calculate all the conditional probabilities (also names likelihoods) used to predict the label for the instance `{\"sex\" : 1, \"island\" : 2}` in Joint Bayes. (*Hint*: panda's query function might provide itself useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1643835616438356, 0.0, 0.0]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def jb_likelihoods(df, sex, island):\n",
    "    cond_prob = list()\n",
    "    for trg in np.unique(df['species']):\n",
    "        filtered = df.query(\"species == @trg\")\n",
    "        likelihoods = filtered.query(\"sex == @sex and island == @island\")\n",
    "        cond_prob.append(len(likelihoods) / len(filtered))\n",
    "    return cond_prob\n",
    "\n",
    "jb_likelihoods(penguin_dataset, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Calculate the aposteriori probabilities of the labels and decide which label will Joint Bayes predict for the instance. Use the conditional and prior probabilities (*not* the logarithmic values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def jb_aposteriori(df, sex, island):\n",
    "    prior = prior_probability_target(df, 'species')\n",
    "    prior = list(map(lambda x: math.exp(x), prior))\n",
    "    likelihoods = jb_likelihoods(df, sex, island)\n",
    "\n",
    "    aposteriori = list()\n",
    "    for trg in np.unique(df['species']):\n",
    "        aposteriori.append( prior[trg-1] * likelihoods[trg - 1] )\n",
    "\n",
    "    # print(aposteriori)\n",
    "    return aposteriori.index( max(aposteriori) ) + 1\n",
    "\n",
    "\n",
    "jb_aposteriori(penguin_dataset, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. **Joint Bayes implemenation**: write a function called `joint_bayes` that takes two arguments:\n",
    "- `df`: the dataframe which will be used for training\n",
    "- `index_target`: the index of the column associated with the target feature\n",
    "\n",
    "The function should return a dictionary with the following fields:\n",
    "- `prior_probs`: the prior probabilities (the probability of the labels)\n",
    "- `likelihoods`: a n x m array, where n - the number of labels; m - the number of combination between the values of the features; each label will have assigned a list containing the joint probability P(feature_1 = value_1, feature_2 = value_2, ...,  feature_n = value_n | target_feature = label)\n",
    "- `n_classes`: the number of labels\n",
    "- `n_feature_classes`: a vector that contains the number of unique values for each attribute\n",
    "- `classes`: the name of the labels (the values of the target feature).\n",
    "\n",
    "*Hint*: check the imports from the first cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_bayes(df, index_target = -1):\n",
    "    target = df.columns[index_target]\n",
    "    output = dict()\n",
    "    output['prior_probs'] = list(map(lambda x: math.exp(x), prior_probability_target(df, target)))\n",
    "    output['n_classes'] = len(np.unique(df[target]))\n",
    "    output['n_feature_classes'] = [ list(np.unique(df[col])) for col in df.columns[:-1] ]\n",
    "    output['classes'] = list(np.unique(df[target]))\n",
    "\n",
    "    likelihoods = dict()\n",
    "    for sex in np.unique(df['sex']):\n",
    "        for island in np.unique(df['island']):\n",
    "            likelihoods[(sex, island)] = dict()\n",
    "            lk = jb_likelihoods(df, sex, island)\n",
    "            for trg in np.unique(df[target]):\n",
    "                likelihoods[(sex, island)][trg] = lk[trg-1]\n",
    "\n",
    "    output['likelihoods'] = likelihoods\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Train the Joint Bayes algorithm on the discrete penguin dataset. Print the obtained dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': [1, 2, 3],\n",
      " 'likelihoods': {(0, 0): {1: 0.1506849315068493, 2: 0.0, 3: 0.5126050420168067},\n",
      "                 (0, 1): {1: 0.1917808219178082, 2: 0.5, 3: 0.0},\n",
      "                 (0, 2): {1: 0.15753424657534246, 2: 0.0, 3: 0.0},\n",
      "                 (1, 0): {1: 0.1506849315068493,\n",
      "                          2: 0.0,\n",
      "                          3: 0.48739495798319327},\n",
      "                 (1, 1): {1: 0.18493150684931506, 2: 0.5, 3: 0.0},\n",
      "                 (1, 2): {1: 0.1643835616438356, 2: 0.0, 3: 0.0}},\n",
      " 'n_classes': 3,\n",
      " 'n_feature_classes': [[0, 1], [0, 1, 2]],\n",
      " 'prior_probs': [0.43843843843843844, 0.20420420420420418, 0.35735735735735735]}\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "jb = joint_bayes(discrete_penguin_dataset)\n",
    "pprint(jb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Similarly to Naive Bayes, write the functions used to predict the aposteriori probabilities, the label and the accuracy of the Joint Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jb_predict_prob(jb_dict, X):\n",
    "    prior = jb_dict['prior_probs']\n",
    "    prob = list()\n",
    "    for trg in jb_dict['classes']:\n",
    "        prob.append( prior[trg-1] * jb_dict['likelihoods'][(X[0], X[1])][trg] )\n",
    "    return prob\n",
    "\n",
    "\n",
    "def jb_predict(jb_dict, X):\n",
    "    predicted_prob = jb_predict_prob(jb_dict, X)\n",
    "    return predicted_prob.index( max(predicted_prob) ) + 1\n",
    "\n",
    "\n",
    "def jb_score(jb_dict, X, y):\n",
    "    ok = 0\n",
    "    for index in range(len(X)):\n",
    "        rez = jb_predict(jb_dict, list(X.loc[index]))\n",
    "        if rez == y.loc[index]:\n",
    "            ok += 1\n",
    "    return ok\n",
    "\n",
    "# jb_predict(jb, [1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "17. Calculate the training accuracy of your Joint Bayes algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7027027027027027"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "X = discrete_penguin_dataset[list(discrete_penguin_dataset.columns[:-1])]\n",
    "y = discrete_penguin_dataset[discrete_penguin_dataset.columns[-1]]\n",
    "jb_score(jb, X, y)/len(discrete_penguin_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. kNN (2 points; 0.15 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will use the entire `penguin_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the Euclidean distance between the test instance `{\"sex\" : 1, \"island\" : 2, \"bill_length\" : 20, \"bill_depth\" : 40, \"flipper_length\" : 355, \"body_mass\" : 855}` and the instances from the dataset. Store the values in an object called `instance_distance`. Print the average distance. (*Hint*: check the norm function from the numpy package.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3356.1302586089714"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def distances(dataset, test, p):\n",
    "    instance_distance = list(norm(dataset[list(dataset.columns[:-1])] - test,p,axis=1))\n",
    "    for index in range(len(instance_distance)):\n",
    "        instance_distance[index] = (dataset.loc[index]['species'], instance_distance[index] + 10**(-10))\n",
    "    instance_distance.sort(key=lambda x: x[1])\n",
    "    return instance_distance\n",
    "    \n",
    "np.mean(list(map(lambda x: x[1], distances(penguin_dataset, [1, 2, 20, 40, 355, 855], 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the `5` nearest neighbours of the test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.0, 1852.5296677787296),\n",
       " (1.0, 2002.5142621215962),\n",
       " (1.0, 2002.7792714127038),\n",
       " (1.0, 2052.0207016500613),\n",
       " (1.0, 2052.058200929109)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "first_five = distances(penguin_dataset, [1, 2, 20, 40, 355, 855], 2)[:5]\n",
    "first_five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Determine the probabilities of the labels that kNN would assign for this test instance (`k = 5`). Which label has the highest probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.2, 0.0]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def prob_no_weights(dataset, target, k, test_x, p):\n",
    "    first_k = distances(penguin_dataset, test_x, p)[:k]\n",
    "    prob_labels = [0] * len(np.unique(dataset[target]))\n",
    "    for index in range(len(first_k)):\n",
    "        label = int(first_k[index][0])\n",
    "        prob_labels[label- 1] = prob_labels[label - 1] + 1\n",
    "\n",
    "    prob_labels = list(map(lambda x: x / k, prob_labels))\n",
    "    return prob_labels\n",
    "\n",
    "prob_no_weights(penguin_dataset, 'species', 5, [1, 2, 20, 40, 355, 855], 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Suppose that kNN gives for each neighbour a weight that is equal to the inverse of the distance. Print the changed probabilities, as well as the label predicted by kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7852063478657886, 0.21479365213421137, 0.0]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prob_with_weights(dataset, target, k, test_x, p):\n",
    "    first_k = distances(penguin_dataset, test_x, p)[:k]\n",
    "    prob_labels = [0] * len(np.unique(dataset[target]))\n",
    "    weights = 0\n",
    "    for index in range(len(first_k)):\n",
    "        label = int(first_k[index][0])\n",
    "        distance = first_k[index][1]\n",
    "        prob_labels[label- 1] = prob_labels[label - 1] + distance**(-1)\n",
    "        weights = weights + distance**(-1)\n",
    "\n",
    "    prob_labels = list(map(lambda x: x / weights, prob_labels))\n",
    "    return prob_labels\n",
    "\n",
    "prob_with_weights(penguin_dataset, 'species', 5, [1, 2, 20, 40, 355, 855], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **k-NN implementation**: Create a function `knn_predict_prob` that will take six arguments:\n",
    "- `df`: the dataframe containing the features and the target feature\n",
    "- `test_x`: a list with the attributes observed for *one* instance\n",
    "- `k`: the number of nearest neighbours\n",
    "- `use_weights`: boolean value that indicates whether to assign weights based on the inverse of the distance or not\n",
    "- `p`: either an integer, indicating the order of the Minkowski distance (p=2 is the equivalent for Euclidean) or a custom distance function\n",
    "- `index_target`: the index of the column that contains the labels of the target feature\n",
    "\n",
    "The function should:\n",
    "- calculate the distance between `test_x` and the observations from the dataset\n",
    "- extract the k-nearest neighbours\n",
    "- calculate the weight for each label\n",
    "- normalize the weights to become probabilities\n",
    "- return the probability vector, that will indicate the probability of the instance `test_x` to have the label `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7852063478657886, 0.21479365213421137, 0.0]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knn_predict_prob(df, test_x, k, use_weights = True, p = 2, index_target = -1):\n",
    "   target = df.columns[index_target]\n",
    "   if use_weights:\n",
    "      return prob_with_weights(df, target, k, test_x, p)\n",
    "   else:\n",
    "      return prob_no_weights(df, target, k, test_x, p)\n",
    "\n",
    "knn_predict_prob(penguin_dataset, [1, 2, 20, 40, 355, 855], 5, True, 2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a function that, based on the probabilities calculated above, returns the label with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knn_predict(df, test_x, k, use_weights = True, p = 2, index_target = -1):\n",
    "    probabilities = knn_predict_prob(df, test_x, k, use_weights, p, index_target)\n",
    "    max_value = max(probabilities)\n",
    "    return probabilities.index(max_value) + 1\n",
    "\n",
    "knn_predict(penguin_dataset, [1, 2, 20, 40, 355, 855], 5, True, 2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Calculate the probabilities and the predicted label for the instance from exercise 1 using the following configurations:\n",
    "- `k = 11, unweighted, Euclidean distance`\n",
    "- `k = 11, weighted, Euclidean distance`\n",
    "- `k = 11, unweighted, Manhattan distance`\n",
    "- `k = 11, weighted, Manhattan distance`\n",
    "\n",
    "Compare your results with the results obtained by the `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 11, unweighted, Euclidean distance: [0.8181818181818182, 0.18181818181818182, 0.0]\n",
      "k = 11, weighted, Euclidean distance: [0.8081241299001588, 0.19187587009984078, 0.0]\n",
      "k = 11, unweighted, Manhattan distance: [0.8181818181818182, 0.18181818181818182, 0.0]\n",
      "k = 11, weighted, Manhattan distance: [0.8094697299455478, 0.19053027005445233, 0.0]\n",
      "[[0.66666667 0.33333333 0.        ]]\n",
      "0.924924924924925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aiaco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\aiaco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.80812413, 0.19187587, 0.        ]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "print(f'k = 11, unweighted, Euclidean distance: {knn_predict_prob(penguin_dataset, [1, 2, 20, 40, 355, 855], 11, False, 2, -1)}')\n",
    "print(f'k = 11, weighted, Euclidean distance: {knn_predict_prob(penguin_dataset, [1, 2, 20, 40, 355, 855], 11, True, 2, -1)}')\n",
    "print(f'k = 11, unweighted, Manhattan distance: {knn_predict_prob(penguin_dataset, [1, 2, 20, 40, 355, 855], 11, False, 1, -1)}')\n",
    "print(f'k = 11, weighted, Manhattan distance: {knn_predict_prob(penguin_dataset, [1, 2, 20, 40, 355, 855], 11, True, 1, -1)}')\n",
    "\n",
    "\n",
    "\n",
    "X = penguin_dataset[list(penguin_dataset.columns[:-1])]\n",
    "y = penguin_dataset[penguin_dataset.columns[-1]]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3, p=1).fit(X,y)\n",
    "print(knn.predict_proba([[1, 2, 20, 40, 355, 855]]))\n",
    "print(knn.score(X,y))\n",
    "\n",
    "knn_weights = KNeighborsClassifier(n_neighbors=11, weights=\"distance\").fit(X,y)\n",
    "knn_weights.predict_proba([[1, 2, 20, 40, 355, 855]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write a function that calculates the accuracy of the kNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.924924924924925"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knn_score(df, test_x, test_y, k, use_weights = True, p = 2, index_target = -1):\n",
    "    count = 0\n",
    "\n",
    "    for index in range(len(test_x)):\n",
    "        attr = list(test_x.loc[index])\n",
    "        trg = test_y.loc[index]\n",
    "        predicted = knn_predict(df, attr, k, use_weights, p, index_target)\n",
    "        if predicted == trg:\n",
    "            count += 1\n",
    "    return count/len(test_y)\n",
    "\n",
    "test_x = penguin_dataset[list(penguin_dataset.columns[:-1])]\n",
    "test_y = penguin_dataset[penguin_dataset.columns[-1]]\n",
    "knn_score(penguin_dataset, test_x, test_y, 3, False, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is the training accuracy of the unweighted kNN when k varies from 3 to 15? (use only odd numbers). Does adding the weight / changing the distance metric improve the score? Justify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "k = 3 p = 1: 1.0\n",
      "k = 3 p = 2: 1.0\n",
      "k = 5 p = 1: 1.0\n",
      "k = 5 p = 2: 1.0\n",
      "k = 7 p = 1: 1.0\n",
      "k = 7 p = 2: 1.0\n",
      "k = 9 p = 1: 1.0\n",
      "k = 9 p = 2: 1.0\n",
      "k = 11 p = 1: 1.0\n",
      "k = 11 p = 2: 1.0\n",
      "k = 13 p = 1: 1.0\n",
      "k = 13 p = 2: 1.0\n",
      "k = 15 p = 1: 1.0\n",
      "k = 15 p = 2: 1.0\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "# print(\"without\")\n",
    "# for k in range(3,16,2):\n",
    "#     print(f'k = {k} p = 1: {knn_score(penguin_dataset, test_x, test_y, k, False, 1, -1)}')\n",
    "#     print(f'k = {k} p = 2: {knn_score(penguin_dataset, test_x, test_y, k, False, 2, -1)}')\n",
    "\n",
    "print(\"weights\")\n",
    "for k in range(3,16,2):\n",
    "    print(f'k = {k} p = 1: {knn_score(penguin_dataset, test_x, test_y, k, True, 1, -1)}')\n",
    "    print(f'k = {k} p = 2: {knn_score(penguin_dataset, test_x, test_y, k, True, 2, -1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. AdaBoost (3 points; 0.25 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Testing the performance (1.5 points; 0.1 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise grading\n",
    "\n",
    "| Section | Exercise | Points |\n",
    "| --- | --- | --- |\n",
    "| I |\t1 | \t0.1 |\n",
    "| I |\t2 | \t0.1 |\n",
    "| I |\t3 | \t0.25 |\n",
    "| I |\t4 | \t0.25 |\n",
    "| I |\t5 | \t0.15 |\n",
    "| I |\t6 | \t0.65 |\n",
    "| I |\t7 | \t0.1 |\n",
    "| I |\t8 | \t0.25 |\n",
    "| I |\t9 | \t0.15 |\n",
    "| I |\t10 | \t0.1 |\n",
    "| I |\t11 | \t0.1 |\n",
    "| I |\t12 | \t0.1 |\n",
    "| I |\t13 | \t0.15 |\n",
    "| I |\t14 | \t0.65 |\n",
    "| I |\t15 | \t0.1 |\n",
    "| I |\t16 | \t0.2 |\n",
    "| I |\t17 | \t0.1 |\n",
    "|II | \t1 | \t0.2 |\n",
    "|II | \t2 | \t0.1 |\n",
    "|II | \t3 | \t0.15 |\n",
    "|II | \t4 | \t0.2 |\n",
    "|II | \t5 | \t0.72 |\n",
    "|II | \t6 | \t0.1 |\n",
    "|II | \t7 | \t0.28 |\n",
    "|II | \t8 | \t0.1 |\n",
    "|II | \t9 | \t0.15 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59c2697042bf23f669049604bc51c7fdaddd84c684e590656d4c80c751d79f30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
