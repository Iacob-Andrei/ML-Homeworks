{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Machine Learning, Fall 2022\n",
    "\n",
    "Deadline: 11th of December 2022, 23:59\n",
    "\n",
    "To do this project you have to complete this Jupyter notebook and send it via Discord.\n",
    "\n",
    "The total number of points allocated for this project is 10.\n",
    "\n",
    "You will need the following modules to solve the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sex  island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0    0       2            39.1           18.7              181.0       3750.0   \n",
      "1    1       2            39.5           17.4              186.0       3800.0   \n",
      "2    1       2            40.3           18.0              195.0       3250.0   \n",
      "3    1       2            36.7           19.3              193.0       3450.0   \n",
      "4    0       2            39.3           20.6              190.0       3650.0   \n",
      "\n",
      "   species  \n",
      "0        1  \n",
      "1        1  \n",
      "2        1  \n",
      "3        1  \n",
      "4        1  \n",
      "   sex  island  species\n",
      "0    0       2        1\n",
      "1    1       2        1\n",
      "2    1       2        1\n",
      "3    1       2        1\n",
      "4    0       2        1\n"
     ]
    }
   ],
   "source": [
    "penguin_dataset = pd.read_csv(\"data/penguins_filtered.csv\")\n",
    "\n",
    "penguin_dataset = penguin_dataset.replace({\n",
    "    \"Adelie\": 1,\n",
    "    \"Chinstrap\" : 2,\n",
    "    \"Gentoo\": 3,\n",
    "    \"male\" : 0,\n",
    "    \"female\" : 1,\n",
    "    \"Biscoe\" : 0,\n",
    "    \"Dream\" : 1,\n",
    "    \"Torgersen\" : 2})\n",
    "\n",
    "discrete_penguin_dataset = penguin_dataset[[\"sex\", \"island\", \"species\"]]\n",
    "print(penguin_dataset.head())\n",
    "print(discrete_penguin_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Naive and Joint Bayes (3.5 points; 0.15 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: Be careful at what Naive Bayes class you use from `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the prior probabilities for the target feature. Transform them by applying the natural logarithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.8245358682721073, -1.5886347848043372, -1.0290189968689145]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def prior_probability_target(dataset, target):\n",
    "    prob = [ list(dataset[target]).count(val) / len(dataset[target]) for val in np.unique(dataset[target])]\n",
    "    prob = list(map(lambda x: math.log(x), prob))\n",
    "    return prob\n",
    "\n",
    "prior_probability_target(penguin_dataset, 'species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write the formulas used to calculate the maximum aposteriori probability using Naive Bayes and Joint Bayes. Use the names of the variables from the discrete penguin dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer here*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find and calculate the logarithm of all the conditional probabilities (also names likelihoods) used to predict the label for the instance `{\"sex\" : 1, \"island\" : 2}` in Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiaco\\AppData\\Local\\Temp\\ipykernel_10448\\3861210417.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  probabilities.append([np.log(sex_count/species_count), np.log(island_count/species_count)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-0.6931471805599453, -1.133459019998278],\n",
       " [-0.6931471805599453, -inf],\n",
       " [-0.7186804825651101, -inf]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def likelihoods(dataset, target, sex_value, island_value):\n",
    "    probabilities = list()\n",
    "\n",
    "    for species_value in np.unique(dataset[target]):\n",
    "        ds = dataset[dataset[target] == species_value].reset_index(drop=True)\n",
    "\n",
    "        sex_count = list(ds['sex']).count(sex_value)\n",
    "        island_count = list(ds['island']).count(island_value)\n",
    "        species_count = list(dataset[target]).count(species_value)\n",
    "\n",
    "        probabilities.append([np.log(sex_count/species_count), np.log(island_count/species_count)])\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "likelihoods(discrete_penguin_dataset, 'species', 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Why does the results contain infinity? Fix the calculation by using the Laplace Smoothing with `alpha = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.6931471805599453, -1.1327452950375683],\n",
       " [-0.6931471805599453, -4.2626798770413155],\n",
       " [-0.7182531016910216, -4.804021044733257]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def likelihoods_laplace(dataset, target, sex_value, island_value, alpha = 1):\n",
    "    probabilities = list()\n",
    "\n",
    "    for species_value in np.unique(dataset[target]):\n",
    "        ds = dataset[dataset[target] == species_value].reset_index(drop=True)\n",
    "\n",
    "        sex_count = list(ds['sex']).count(sex_value) + alpha\n",
    "        island_count = list(ds['island']).count(island_value) + alpha\n",
    "        species_count = len(ds['species'])\n",
    "\n",
    "        probabilities.append ([math.log(sex_count    / (species_count + alpha*len(np.unique(dataset['sex'])))),\n",
    "                               math.log(island_count / (species_count + alpha*len(np.unique(dataset['island']))))] )\n",
    "    return probabilities\n",
    "\n",
    "likelihoods_laplace(discrete_penguin_dataset, 'species', 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Calculate the aposteriori probabilities of the labels and decide which label will Naive Bayes predict for the instance. Use only the logarithm values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.9609956286737821), (2, 0.019568798053176163), (3, 0.01943557327304175)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def aposteriori(dataset, target, sex_value, island_value):\n",
    "    priori = prior_probability_target(dataset, target)\n",
    "    likelihood = likelihoods_laplace(dataset, target, sex_value, island_value)\n",
    "    \n",
    "    prod = list()\n",
    "    for index in range(len(priori)):\n",
    "        prod.append( (index + 1, math.exp(sum(likelihood[index], priori[index])) ) )\n",
    "\n",
    "    suma = 0\n",
    "    for index in range(len(prod)):\n",
    "        suma += prod[index][1]\n",
    "\n",
    "    prod = list(map(lambda x: x[1]/suma, prod))\n",
    "    for index in range(3):\n",
    "        prod[index] = (index+1, prod[index])\n",
    "    return prod\n",
    "    \n",
    "aposteriori(discrete_penguin_dataset, 'species', 1 , 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Naive Bayes implemenation**: write a function called `naive_bayes` that takes three arguments:\n",
    "- `df`: the dataframe which will be used for training\n",
    "- `index_target`: the index of the column associated with the target feature\n",
    "- `alpha`: the parameter used for Laplace Smoothing\n",
    "\n",
    "The function should return a dictionary with the following fields:\n",
    "- `log_prior`: the logarithmic values of the prior probabilities (the probability of the labels)\n",
    "- `log_likelihoods`: a n x m x t array, where n - the number of features; m - the number of labels; t - the number of values for a feature; this array will contain the logarithmic values of the likelihoods (P(feature = value | target_feature = label))\n",
    "- `n_classes`: the number of labels\n",
    "- `n_feature_classes`: a vector that contains the number of unique values for each attribute\n",
    "- `classes`: the name of the labels (the values of the target feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': 3,\n",
      " 'log_likelihoods': {'island': {0: {1: -1.1972838161751393,\n",
      "                                    2: -4.2626798770413155,\n",
      "                                    3: -0.016529301951210582},\n",
      "                                1: {1: -0.9785946152103099,\n",
      "                                    2: -0.028573372444056,\n",
      "                                    3: -4.804021044733257},\n",
      "                                2: {1: -1.1327452950375683,\n",
      "                                    2: -4.2626798770413155,\n",
      "                                    3: -4.804021044733257}},\n",
      "                     'sex': {0: {1: -0.6931471805599453,\n",
      "                                 2: -0.6931471805599453,\n",
      "                                 3: -0.6686561605516496},\n",
      "                             1: {1: -0.6931471805599453,\n",
      "                                 2: -0.6931471805599453,\n",
      "                                 3: -0.7182531016910216}}},\n",
      " 'log_prior': [-0.8245358682721073, -1.5886347848043372, -1.0290189968689145],\n",
      " 'n_classes': [1, 2, 3],\n",
      " 'n_feature_classes': [[0, 1], [0, 1, 2]]}\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes(df, index_target = -1, alpha = 1):\n",
    "    target = df.columns[index_target]\n",
    "    output = dict()\n",
    "    output['log_prior'] = prior_probability_target(df, target)\n",
    "    output['n_classes'] = list(np.unique(df[target]))\n",
    "    output['n_feature_classes'] = [ list(np.unique(df[col])) for col in df.columns[:-1] ]\n",
    "    output['classes'] = len(np.unique(df[target]))\n",
    "    \n",
    "    sex_list = dict()\n",
    "    for sex in output['n_feature_classes'][0]:\n",
    "        prob = likelihoods_laplace(df, target, sex, 0, alpha)\n",
    "        sex_list[sex] = {1: prob[0][0], 2: prob[1][0], 3: prob[2][0]}\n",
    "\n",
    "    island_list = dict()\n",
    "    for island in output['n_feature_classes'][1]:\n",
    "        prob = likelihoods_laplace(df, target, 0, island, alpha)\n",
    "        island_list[island] = {1: prob[0][1], 2: prob[1][1], 3: prob[2][1]}\n",
    "\n",
    "    \n",
    "    output['log_likelihoods'] = {'sex': sex_list, 'island': island_list}\n",
    "    return output\n",
    "\n",
    "pprint(naive_bayes(discrete_penguin_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Train the discrete penguin dataset using your version of Naive Bayes and sklearn's. Compare the values of the parameters.(Be careful at what type of Naive Bayes classifier you pick from sklearn!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.69314718, -0.69314718],\n",
      "       [-0.69314718, -0.69314718],\n",
      "       [-0.66865616, -0.7182531 ]]),\n",
      " array([[-1.19728382, -0.97859462, -1.1327453 ],\n",
      "       [-4.26267988, -0.02857337, -4.26267988],\n",
      "       [-0.0165293 , -4.80402104, -4.80402104]])]\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "X = discrete_penguin_dataset[['sex','island']]\n",
    "y = discrete_penguin_dataset['species']\n",
    "cl = CategoricalNB(alpha=1).fit(X, y)\n",
    "\n",
    "new_messages = pd.DataFrame(\n",
    "  [(1, 2)],\n",
    "columns = ['sex','island'])\n",
    "\n",
    "pprint(cl.feature_log_prob_)\n",
    "# print(cl.predict(new_messages), '\\n', cl.predict_proba(new_messages))\n",
    "#print(cl.predict_proba(new_messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a function named `nb_predict_prob` that uses the log probabilities calculated by Naive Bayes to infer the aposteriori probability of a new instance `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.6504283438696206, -6.544461842405598, -6.551293143293193]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nb_predict_prob(nb_dict, X, use_log = False):\n",
    "    prob = list()\n",
    "    for trg in nb_dict['n_classes']:\n",
    "        prb = nb_dict['log_prior'][trg-1] + nb_dict['log_likelihoods']['sex'][X[0]][trg] + nb_dict['log_likelihoods']['island'][X[1]][trg]\n",
    "        prob.append(prb)\n",
    "\n",
    "    return prob\n",
    "\n",
    "nb_predict_prob(naive_bayes(discrete_penguin_dataset), [1,2], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Create a function that does the Naive Bayes prediction using the Maximum Aposteriori Probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nb_predict(nb_dict, X):\n",
    "    prob = nb_predict_prob(nb_dict, X)\n",
    "    return prob.index(max(prob)) + 1\n",
    "\n",
    "nb_predict(naive_bayes(discrete_penguin_dataset), [1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a function that calculate the accuracy of the trained model on a set of instances `X` with known labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_score(nb_dict, X, y):\n",
    "    ok = 0\n",
    "    for index in range(len(X)):\n",
    "        rez = nb_predict(nb_dict, list(X.loc[index]))\n",
    "        if rez == y.loc[index]:\n",
    "            ok += 1\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Calculate the training accuracy of your Naive Bayes algorithm. Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7027027027027027"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "X = discrete_penguin_dataset[list(discrete_penguin_dataset.columns[:-1])]\n",
    "y = discrete_penguin_dataset[discrete_penguin_dataset.columns[-1]]\n",
    "nb_score(naive_bayes(discrete_penguin_dataset), X, y)/len(discrete_penguin_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Find and calculate all the conditional probabilities (also names likelihoods) used to predict the label for the instance `{\"sex\" : 1, \"island\" : 2}` in Joint Bayes. (*Hint*: panda's query function might provide itself useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1643835616438356, 0.0, 0.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def jb_likelihoods(df, sex, island):\n",
    "    cond_prob = list()\n",
    "    for trg in np.unique(df['species']):\n",
    "        filtered = df.query(\"species == @trg\")\n",
    "        likelihoods = filtered.query(\"sex == @sex and island == @island\")\n",
    "        cond_prob.append(len(likelihoods) / len(filtered))\n",
    "    return cond_prob\n",
    "\n",
    "jb_likelihoods(penguin_dataset, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Calculate the aposteriori probabilities of the labels and decide which label will Joint Bayes predict for the instance. Use the conditional and prior probabilities (*not* the logarithmic values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def jb_aposteriori(df, sex, island):\n",
    "    prior = prior_probability_target(df, 'species')\n",
    "    prior = list(map(lambda x: math.exp(x), prior))\n",
    "    likelihoods = jb_likelihoods(df, sex, island)\n",
    "\n",
    "    aposteriori = list()\n",
    "    for trg in np.unique(df['species']):\n",
    "        aposteriori.append( prior[trg-1] * likelihoods[trg - 1] )\n",
    "\n",
    "    # print(aposteriori)\n",
    "    return aposteriori.index( max(aposteriori) ) + 1\n",
    "\n",
    "\n",
    "jb_aposteriori(penguin_dataset, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. **Joint Bayes implemenation**: write a function called `joint_bayes` that takes two arguments:\n",
    "- `df`: the dataframe which will be used for training\n",
    "- `index_target`: the index of the column associated with the target feature\n",
    "\n",
    "The function should return a dictionary with the following fields:\n",
    "- `prior_probs`: the prior probabilities (the probability of the labels)\n",
    "- `likelihoods`: a n x m array, where n - the number of labels; m - the number of combination between the values of the features; each label will have assigned a list containing the joint probability P(feature_1 = value_1, feature_2 = value_2, ...,  feature_n = value_n | target_feature = label)\n",
    "- `n_classes`: the number of labels\n",
    "- `n_feature_classes`: a vector that contains the number of unique values for each attribute\n",
    "- `classes`: the name of the labels (the values of the target feature).\n",
    "\n",
    "*Hint*: check the imports from the first cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_bayes(df, index_target = -1):\n",
    "    target = df.columns[index_target]\n",
    "    output = dict()\n",
    "    output['prior_probs'] = list(map(lambda x: math.exp(x), prior_probability_target(df, target)))\n",
    "    output['n_classes'] = len(np.unique(df[target]))\n",
    "    output['n_feature_classes'] = [ list(np.unique(df[col])) for col in df.columns[:-1] ]\n",
    "    output['classes'] = list(np.unique(df[target]))\n",
    "\n",
    "    likelihoods = dict()\n",
    "    for sex in np.unique(df['sex']):\n",
    "        for island in np.unique(df['island']):\n",
    "            likelihoods[(sex, island)] = dict()\n",
    "            lk = jb_likelihoods(df, sex, island)\n",
    "            for trg in np.unique(df[target]):\n",
    "                likelihoods[(sex, island)][trg] = lk[trg-1]\n",
    "\n",
    "    output['likelihoods'] = likelihoods\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Train the Joint Bayes algorithm on the discrete penguin dataset. Print the obtained dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': [1, 2, 3],\n",
      " 'likelihoods': {(0, 0): {1: 0.1506849315068493, 2: 0.0, 3: 0.5126050420168067},\n",
      "                 (0, 1): {1: 0.1917808219178082, 2: 0.5, 3: 0.0},\n",
      "                 (0, 2): {1: 0.15753424657534246, 2: 0.0, 3: 0.0},\n",
      "                 (1, 0): {1: 0.1506849315068493,\n",
      "                          2: 0.0,\n",
      "                          3: 0.48739495798319327},\n",
      "                 (1, 1): {1: 0.18493150684931506, 2: 0.5, 3: 0.0},\n",
      "                 (1, 2): {1: 0.1643835616438356, 2: 0.0, 3: 0.0}},\n",
      " 'n_classes': 3,\n",
      " 'n_feature_classes': [[0, 1], [0, 1, 2]],\n",
      " 'prior_probs': [0.43843843843843844, 0.20420420420420418, 0.35735735735735735]}\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "jb = joint_bayes(discrete_penguin_dataset)\n",
    "pprint(jb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Similarly to Naive Bayes, write the functions used to predict the aposteriori probabilities, the label and the accuracy of the Joint Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jb_predict_prob(jb_dict, X):\n",
    "    prior = jb_dict['prior_probs']\n",
    "    prob = list()\n",
    "    for trg in jb_dict['classes']:\n",
    "        prob.append( prior[trg-1] * jb_dict['likelihoods'][(X[0], X[1])][trg] )\n",
    "    return prob\n",
    "\n",
    "\n",
    "def jb_predict(jb_dict, X):\n",
    "    predicted_prob = jb_predict_prob(jb_dict, X)\n",
    "    return predicted_prob.index( max(predicted_prob) ) + 1\n",
    "\n",
    "\n",
    "def jb_score(jb_dict, X, y):\n",
    "    ok = 0\n",
    "    for index in range(len(X)):\n",
    "        rez = jb_predict(jb_dict, list(X.loc[index]))\n",
    "        if rez == y.loc[index]:\n",
    "            ok += 1\n",
    "    return ok\n",
    "\n",
    "# jb_predict(jb, [1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "17. Calculate the training accuracy of your Joint Bayes algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7027027027027027"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "X = discrete_penguin_dataset[list(discrete_penguin_dataset.columns[:-1])]\n",
    "y = discrete_penguin_dataset[discrete_penguin_dataset.columns[-1]]\n",
    "jb_score(jb, X, y)/len(discrete_penguin_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. kNN (2 points; 0.15 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will use the entire `penguin_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the Euclidean distance between the test instance `{\"sex\" : 1, \"island\" : 2, \"bill_length\" : 20, \"bill_depth\" : 40, \"flipper_length\" : 355, \"body_mass\" : 855}` and the instances from the dataset. Store the values in an object called `instance_distance`. Print the average distance. (*Hint*: check the norm function from the numpy package.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3356.1302586089714"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def distances(dataset, test, p):\n",
    "    instance_distance = list(norm(dataset[list(dataset.columns[:-1])] - test,p,axis=1))\n",
    "    for index in range(len(instance_distance)):\n",
    "        instance_distance[index] = (dataset.loc[index]['species'], instance_distance[index] + 10**(-10))\n",
    "    instance_distance.sort(key=lambda x: x[1])\n",
    "    return instance_distance\n",
    "    \n",
    "np.mean(list(map(lambda x: x[1], distances(penguin_dataset, [1, 2, 20, 40, 355, 855], 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the `5` nearest neighbours of the test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.0, 1852.5296677787296),\n",
       " (1.0, 2002.5142621215962),\n",
       " (1.0, 2002.7792714127038),\n",
       " (1.0, 2052.0207016500613),\n",
       " (1.0, 2052.058200929109)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "first_five = distances(penguin_dataset, [1, 2, 20, 40, 355, 855], 2)[:5]\n",
    "first_five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Determine the probabilities of the labels that kNN would assign for this test instance (`k = 5`). Which label has the highest probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.2, 0.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "def prob_no_weights(dataset, target, k, test_x, p):\n",
    "    first_k = distances(penguin_dataset, test_x, p)[:k]\n",
    "    prob_labels = [0] * len(np.unique(dataset[target]))\n",
    "    for index in range(len(first_k)):\n",
    "        label = int(first_k[index][0])\n",
    "        prob_labels[label- 1] = prob_labels[label - 1] + 1\n",
    "\n",
    "    prob_labels = list(map(lambda x: x / k, prob_labels))\n",
    "    return prob_labels\n",
    "\n",
    "prob_no_weights(penguin_dataset, 'species', 5, [1, 2, 20, 40, 355, 855], 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Suppose that kNN gives for each neighbour a weight that is equal to the inverse of the distance. Print the changed probabilities, as well as the label predicted by kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7852063478657886, 0.21479365213421137, 0.0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prob_with_weights(dataset, target, k, test_x, p):\n",
    "    first_k = distances(penguin_dataset, test_x, p)[:k]\n",
    "    prob_labels = [0] * len(np.unique(dataset[target]))\n",
    "    weights = 0\n",
    "    for index in range(len(first_k)):\n",
    "        label = int(first_k[index][0])\n",
    "        distance = first_k[index][1]\n",
    "        prob_labels[label- 1] = prob_labels[label - 1] + distance**(-1)\n",
    "        weights = weights + distance**(-1)\n",
    "\n",
    "    prob_labels = list(map(lambda x: x / weights, prob_labels))\n",
    "    return prob_labels\n",
    "\n",
    "prob_with_weights(penguin_dataset, 'species', 5, [1, 2, 20, 40, 355, 855], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **k-NN implementation**: Create a function `knn_predict_prob` that will take six arguments:\n",
    "- `df`: the dataframe containing the features and the target feature\n",
    "- `test_x`: a list with the attributes observed for *one* instance\n",
    "- `k`: the number of nearest neighbours\n",
    "- `use_weights`: boolean value that indicates whether to assign weights based on the inverse of the distance or not\n",
    "- `p`: either an integer, indicating the order of the Minkowski distance (p=2 is the equivalent for Euclidean) or a custom distance function\n",
    "- `index_target`: the index of the column that contains the labels of the target feature\n",
    "\n",
    "The function should:\n",
    "- calculate the distance between `test_x` and the observations from the dataset\n",
    "- extract the k-nearest neighbours\n",
    "- calculate the weight for each label\n",
    "- normalize the weights to become probabilities\n",
    "- return the probability vector, that will indicate the probability of the instance `test_x` to have the label `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7852063478657886, 0.21479365213421137, 0.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knn_predict_prob(df, test_x, k, use_weights = True, p = 2, index_target = -1):\n",
    "   target = df.columns[index_target]\n",
    "   if use_weights:\n",
    "      return prob_with_weights(df, target, k, test_x, p)\n",
    "   else:\n",
    "      return prob_no_weights(df, target, k, test_x, p)\n",
    "\n",
    "knn_predict_prob(penguin_dataset, [1, 2, 20, 40, 355, 855], 5, True, 2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a function that, based on the probabilities calculated above, returns the label with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knn_predict(df, test_x, k, use_weights = True, p = 2, index_target = -1):\n",
    "    probabilities = knn_predict_prob(df, test_x, k, use_weights, p, index_target)\n",
    "    max_value = max(probabilities)\n",
    "    return probabilities.index(max_value) + 1\n",
    "\n",
    "knn_predict(penguin_dataset, [1, 2, 20, 40, 355, 855], 5, True, 2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Calculate the probabilities and the predicted label for the instance from exercise 1 using the following configurations:\n",
    "- `k = 11, unweighted, Euclidean distance`\n",
    "- `k = 11, weighted, Euclidean distance`\n",
    "- `k = 11, unweighted, Manhattan distance`\n",
    "- `k = 11, weighted, Manhattan distance`\n",
    "\n",
    "Compare your results with the results obtained by the `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 11, unweighted, Euclidean distance: [0.8181818181818182, 0.18181818181818182, 0.0]\n",
      "k = 11, weighted, Euclidean distance: [0.8081241299001588, 0.19187587009984078, 0.0]\n",
      "k = 11, unweighted, Manhattan distance: [0.8181818181818182, 0.18181818181818182, 0.0]\n",
      "k = 11, weighted, Manhattan distance: [0.8094697299455478, 0.19053027005445233, 0.0]\n",
      "[[0.66666667 0.33333333 0.        ]]\n",
      "0.924924924924925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aiaco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\aiaco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.80812413, 0.19187587, 0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "print(f'k = 11, unweighted, Euclidean distance: {knn_predict_prob(penguin_dataset, [1, 2, 20, 40, 355, 855], 11, False, 2, -1)}')\n",
    "print(f'k = 11, weighted, Euclidean distance: {knn_predict_prob(penguin_dataset, [1, 2, 20, 40, 355, 855], 11, True, 2, -1)}')\n",
    "print(f'k = 11, unweighted, Manhattan distance: {knn_predict_prob(penguin_dataset, [1, 2, 20, 40, 355, 855], 11, False, 1, -1)}')\n",
    "print(f'k = 11, weighted, Manhattan distance: {knn_predict_prob(penguin_dataset, [1, 2, 20, 40, 355, 855], 11, True, 1, -1)}')\n",
    "\n",
    "\n",
    "\n",
    "X = penguin_dataset[list(penguin_dataset.columns[:-1])]\n",
    "y = penguin_dataset[penguin_dataset.columns[-1]]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3, p=1).fit(X,y)\n",
    "print(knn.predict_proba([[1, 2, 20, 40, 355, 855]]))\n",
    "print(knn.score(X,y))\n",
    "\n",
    "knn_weights = KNeighborsClassifier(n_neighbors=11, weights=\"distance\").fit(X,y)\n",
    "knn_weights.predict_proba([[1, 2, 20, 40, 355, 855]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write a function that calculates the accuracy of the kNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.924924924924925"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knn_score(df, test_x, test_y, k, use_weights = True, p = 2, index_target = -1):\n",
    "    count = 0\n",
    "\n",
    "    for index in range(len(test_x)):\n",
    "        attr = list(test_x.loc[index])\n",
    "        trg = test_y.loc[index]\n",
    "        predicted = knn_predict(df, attr, k, use_weights, p, index_target)\n",
    "        if predicted == trg:\n",
    "            count += 1\n",
    "    return count/len(test_y)\n",
    "\n",
    "test_x = penguin_dataset[list(penguin_dataset.columns[:-1])]\n",
    "test_y = penguin_dataset[penguin_dataset.columns[-1]]\n",
    "knn_score(penguin_dataset, test_x, test_y, 3, False, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is the training accuracy of the unweighted kNN when k varies from 3 to 15? (use only odd numbers). Does adding the weight / changing the distance metric improve the score? Justify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here\n",
    "# print(\"without\")\n",
    "# for k in range(3,16,2):\n",
    "#     print(f'k = {k} p = 1: {knn_score(penguin_dataset, test_x, test_y, k, False, 1, -1)}')\n",
    "#     print(f'k = {k} p = 2: {knn_score(penguin_dataset, test_x, test_y, k, False, 2, -1)}')\n",
    "\n",
    "# print(\"weights\")\n",
    "# for k in range(3,16,2):\n",
    "#     print(f'k = {k} p = 1: {knn_score(penguin_dataset, test_x, test_y, k, True, 1, -1)}')\n",
    "#     print(f'k = {k} p = 2: {knn_score(penguin_dataset, test_x, test_y, k, True, 2, -1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. AdaBoost (4.5 points; 0.5 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notes*: \n",
    "- The results might differ from the sklearn implementation\n",
    "- we will use the definition from the Machine Learning course of a weak classifier, which linearly separates an attribute\n",
    "- we try to use classifiers that minimise the error, not the entropy, as ID3\n",
    "- for getting the thresholds / splitting points, check the implementation from the Project 1 (mine or yours, whatever works for you)\n",
    "- remember the usage of the external threshold\n",
    "- some exercises from the course's book contain useful tricks to reduce the number of calculation you make during an iteration\n",
    "- the following implementation is a generalisation (with respect to the number of labels) of the AdaBoost algorithm that was discussed at the course\n",
    "    - suppose the target feature `Y` has `n` labels (i.e. `y1`, `y2` ... `yn`)\n",
    "    - suppose you have a feature `X` and a threshold / splitting point `x`\n",
    "    - on the right side of the separator (so for `X > x`), we have `k1` points with label `y1`, `k2` points with label `y2` and so on\n",
    "    - the label that the weak classifier will predict for the surface `X > x` will be the one with the highest probabilities of the points\n",
    "    - in other words, the label will be $\\displaystyle \\argmax_{y \\in Val(Y)} \\sum_{point \\in X, point > x; \\;\\; label(point) = y} D_t(point)$\n",
    "    - we perform similar calculation for the left side of the separator\n",
    "    - in the end, we will get the two labels that the weak classifier will assign\n",
    "    - the error of the classifier will be the sum of the probabilities of the points that are misclasified\n",
    "- please truncate your results; use 7 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a numpy array named `D1` that contains the initial probability distribution of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.003003 0.003003 0.003003 0.003003 0.003003]\n"
     ]
    }
   ],
   "source": [
    "def get_thresholds(dataset, feature, target_feature):\n",
    "    # sort the unique values of feature\n",
    "    # then check if two consecutive values have at least one different label\n",
    "    feature_values = dict()\n",
    "    unique_feature_values = np.sort(np.unique(dataset[feature]))\n",
    "    thresholds = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        current_value = dataset[feature].iloc[i]\n",
    "        current_target_value = dataset[target_feature].iloc[i]\n",
    "        if current_value in feature_values:\n",
    "            feature_values[current_value].add(current_target_value)\n",
    "        else:\n",
    "            feature_values[current_value] = {current_target_value}\n",
    "\n",
    "    for i in range(1, len(unique_feature_values)):\n",
    "        current_val = unique_feature_values[i]\n",
    "        prev_val = unique_feature_values[i-1]\n",
    "\n",
    "        if (len(feature_values[prev_val]) > 1 and len(feature_values[prev_val] & feature_values[current_val]) > 0) or \\\n",
    "            (len(feature_values[prev_val]) == 1 and len(feature_values[current_val] - feature_values[prev_val]) > 0):\n",
    "            thresholds.append((current_val + prev_val) / 2)\n",
    "    \n",
    "    return thresholds\n",
    "\n",
    "\n",
    "\n",
    "count = penguin_dataset.shape[0]\n",
    "D1 = np.array([round(1/count, 7)] * count)\n",
    "print(D1[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the number of the available weak estimators used in our version of AdaBoost? (include the external threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds = list()\n",
    "for col in penguin_dataset.columns[:-1]:\n",
    "    thresholds += get_thresholds(penguin_dataset, col, penguin_dataset.columns[-1])\n",
    "weak_estimators = 1 + len(thresholds)\n",
    "weak_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For the attribute `flipper_length_mm` and the threshold 206.5, find the sum of probabilities of the three species for both left and right side of the separator. (for example, on the right side we will have probability `a` for `species=1`, `b` for `species=2`, `c` for `species=3`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 0.19219200000000022, 2: 0.43543499999999896, 3: 0.6216209999999976},\n",
       " {1: 0.36936899999999945, 2: 0.3603599999999995, 3: 0.021020999999999998})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weak_classiffier_proba(dataset, attribute, threshold, target, D):\n",
    "    prob_left = dict()\n",
    "    prob_right = dict()\n",
    "\n",
    "    for y in np.unique(dataset[target]):\n",
    "        prob_left[y] = 0\n",
    "        prob_right[y] = 0\n",
    "\n",
    "        for index in range(dataset.shape[0]):\n",
    "            if dataset.loc[index][attribute] < threshold:\n",
    "                if dataset.loc[index][target] != y:\n",
    "                    prob_left[y] += D[index]\n",
    "            else:\n",
    "                if dataset.loc[index][target] != y:\n",
    "                    prob_right[y] += D[index]\n",
    "    return prob_left, prob_right\n",
    "\n",
    "weak_classiffier_proba(penguin_dataset, 'flipper_length_mm', 206.5, 'species', D1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the label that the weak classifier will assign for the right side of the separator? But for the left side?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n"
     ]
    }
   ],
   "source": [
    "left, right = weak_classiffier_proba(penguin_dataset, 'flipper_length_mm', 206.5, 'species', D1)\n",
    "\n",
    "left_label = min(left, key=left.get)\n",
    "right_label = min(right, key=right.get)\n",
    "print(left_label, right_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is the error of this weak classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.213213"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = round(right[right_label] + left[left_label], 7)\n",
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is the label the weak classifier associated with the external split point will predict? What about its error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.5615609999999981, 2: 0.7957949999999964, 3: 0.6426419999999975} \n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "left, right = weak_classiffier_proba(penguin_dataset, 'flipper_length_mm', -1, 'species', D1)\n",
    "right_label = min(right, key=right.get)\n",
    "\n",
    "print(right, '\\n', right_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a function that calculates the error of a weak classifier. Test your function on the classifier mentioned in the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.213213, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "def get_error_classifier(df, feature_index, threshold, target_feature, probs):\n",
    "    left, right = weak_classiffier_proba(df, feature_index, threshold, target_feature, probs)\n",
    "    left_label = min(left, key=left.get)\n",
    "    right_label = min(right, key=right.get)\n",
    "    \n",
    "    return (round(left[left_label] + right[right_label], 7), left_label, right_label)\n",
    "\n",
    "print(get_error_classifier(penguin_dataset, 'flipper_length_mm', 206.5, \"species\", D1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write a function that identifies the weak estimator with the lowest error. Do not forget to include the external split point. What is the best weak estimator that will be chosen at the first iteration of AdaBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 205.5, 1, 3) 0.213213\n"
     ]
    }
   ],
   "source": [
    "# should return a tuple with two elements: the estimator (also a tuple with the index of the feature, the threshold, the label assigned in the left side, the label from the right side) and its error\n",
    "def get_best_weak_estimator(df, target_feature, current_probs):\n",
    "    est1, err1 = None, np.Infinity \n",
    "\n",
    "    for index, col in enumerate(df.columns[:-1]):\n",
    "        thresholds = get_thresholds(df, col, target_feature)\n",
    "        for threshold in thresholds:\n",
    "            err, left_label, right_label = get_error_classifier(df, col, threshold, target_feature, current_probs)\n",
    "            if err < err1:\n",
    "                est1 = (index, threshold, left_label, right_label)\n",
    "                err1 = err\n",
    "\n",
    "    # exterior threshold\n",
    "    err, left_label, right_label = get_error_classifier(df, col, -1, target_feature, current_probs)\n",
    "    if err < err1:\n",
    "        est1 = (index, threshold, left_label, right_label)\n",
    "        err1 = err\n",
    "\n",
    "    return est1, err1\n",
    "\n",
    "est1, eps1 = get_best_weak_estimator(penguin_dataset, \"species\", D1)\n",
    "print(est1, eps1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Calculate the weight `alpha_1` that AdaBoost will assign to this weak classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6528329488563055"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def alpha(err):\n",
    "    return (1/2) * np.log((1-err)/err)\n",
    "    \n",
    "alpha(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Update the probability distribution `D2` that will be used in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000041\n",
      "0.003003015315315315\n"
     ]
    }
   ],
   "source": [
    "D2 = []\n",
    "\n",
    "prob_wrong_classified = D1[0] / (2*eps1)\n",
    "prob_ok_classified = D1[0] / (2*(1-eps1))\n",
    "\n",
    "feature = penguin_dataset.columns[est1[0]]\n",
    "for index in range(len(penguin_dataset)):\n",
    "    y = 0\n",
    "    if penguin_dataset.loc[index][feature] < est1[1]:\n",
    "        y = est1[2]\n",
    "    else:\n",
    "        y = est1[3]\n",
    "    if penguin_dataset.loc[index]['species'] == y:\n",
    "        D2.append(round(prob_ok_classified, 7))\n",
    "    else:\n",
    "        D2.append(round(prob_wrong_classified, 7))\n",
    "\n",
    "print(round(sum(D2),7))\n",
    "print(np.mean(D2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Write a function that performs the update of the probability distribution. Test the function for the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n"
     ]
    }
   ],
   "source": [
    "def update_probs(df, estimator, target_feature, eps, current_probs):\n",
    "    new_probs = []\n",
    "    feature = df.columns[estimator[0]]\n",
    "    for index in range(len(df)):\n",
    "        y = 0\n",
    "        if penguin_dataset.loc[index][feature] < estimator[1]:\n",
    "            y = estimator[2]\n",
    "        else:\n",
    "            y = estimator[3]\n",
    "\n",
    "        if df.loc[index][target_feature] != y:\n",
    "            prob = round(current_probs[index] / (2*eps), 7)\n",
    "        else:\n",
    "            prob = round(current_probs[index] / (2*(1-eps)), 7)\n",
    "\n",
    "        new_probs.append(prob)\n",
    "    return np.array(new_probs)\n",
    "\n",
    "print(np.sum(update_probs(penguin_dataset, est1, \"species\", eps1, D1) - D2 < 1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Write a function that trains the AdaBoost algorithm on a dataset. The function should return five fields:\n",
    "- `estimators`: a list of the weak estimators\n",
    "- `estimators_error`: a list with the errors of the estimators\n",
    "- `estimators_weight`: a list with the weights assigned to each estimator\n",
    "- `n_iters`: the number of iterations\n",
    "- `features`: a list with the features used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimators': [(4, 205.5, 1, 3),\n",
      "                (2, 42.349999999999994, 1, 2),\n",
      "                (1, 0.5, 3, 2),\n",
      "                (3, 16.45, 3, 1),\n",
      "                (2, 44.25, 1, 2)],\n",
      " 'estimators_error': [0.213213, 0.2673105, 0.2304498, 0.2532741, 0.2764647],\n",
      " 'estimators_weight': [0.6528329488563055,\n",
      "                       0.5041555532128549,\n",
      "                       0.6028865685095748,\n",
      "                       0.540612940954006,\n",
      "                       0.4810330949698846],\n",
      " 'features': Index(['sex', 'island', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm',\n",
      "       'body_mass_g'],\n",
      "      dtype='object'),\n",
      " 'n_iters': 5}\n"
     ]
    }
   ],
   "source": [
    "def adaboost(df, target_feature, n_iters):\n",
    "    ab = dict()\n",
    "    ab['estimators'] = list()\n",
    "    ab['estimators_error'] = list()\n",
    "    ab['estimators_weight'] = list()\n",
    "    ab['n_iters'] = n_iters\n",
    "    ab['features'] = df.columns[:-1]\n",
    "\n",
    "    count = penguin_dataset.shape[0]\n",
    "    D = np.array([round(1/count, 7)] * count)\n",
    "    for iter in range(n_iters):\n",
    "        est, eps = get_best_weak_estimator(df, target_feature, D)\n",
    "        ab['estimators'].append(est)\n",
    "        ab['estimators_error'].append(eps)\n",
    "        # ab['estimators_weight'].append(D)\n",
    "        ab['estimators_weight'].append(alpha(eps))\n",
    "        # print(sum(D))\n",
    "\n",
    "        D = update_probs(df, est, target_feature, eps, D)\n",
    "    return ab\n",
    "\n",
    "my_ab = adaboost(penguin_dataset, \"species\", 5)\n",
    "pprint(my_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Write a function that does the AdaBoost prediction of an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adaboost_predict(adaboost_model, X):\n",
    "    probs = dict()\n",
    "    probs[1] = probs[2] = probs[3] = 0\n",
    "    \n",
    "    for index in range(adaboost_model['n_iters']):\n",
    "        est = adaboost_model['estimators'][index]\n",
    "        # alp = alpha(adaboost_model['estimators_error'][index])\n",
    "        alp = adaboost_model['estimators_weight'][index]\n",
    "        if X[est[0]] < est[1]:\n",
    "            probs[est[2]] += alp\n",
    "        else:\n",
    "            probs[est[3]] += alp\n",
    "\n",
    "    return max(probs, key=probs.get)\n",
    "\n",
    "adaboost_predict(my_ab, penguin_dataset[penguin_dataset.columns[:-1]].iloc[0]), penguin_dataset[\"species\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Write a function that calculates the accuracy of AdaBoost on a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972972972972973\n"
     ]
    }
   ],
   "source": [
    "def adaboost_score(adaboost_model, X, y):\n",
    "    correct = 0\n",
    "    for index in range(len(X)):\n",
    "        if adaboost_predict(adaboost_model, X[index]) == y[index]:\n",
    "            correct = correct + 1\n",
    "    return correct/len(X)\n",
    "\n",
    "print(adaboost_score(my_ab, penguin_dataset[penguin_dataset.columns[:-1]].to_numpy(), penguin_dataset[\"species\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Draw a plot where you compare the evolution of accuracy of our implementation of AdaBoost and sklearn's. The number of iterations should vary between 1 and 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999989999999949\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "1.0000084000000031\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "1.0000084000000031\n",
      "1.0000015999999965\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "1.0000084000000031\n",
      "1.0000015999999965\n",
      "0.9999993000000005\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "1.0000084000000031\n",
      "1.0000015999999965\n",
      "0.9999993000000005\n",
      "1.0000023000000056\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "1.0000084000000031\n",
      "1.0000015999999965\n",
      "0.9999993000000005\n",
      "1.0000023000000056\n",
      "1.0000068000000013\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "1.0000084000000031\n",
      "1.0000015999999965\n",
      "0.9999993000000005\n",
      "1.0000023000000056\n",
      "1.0000068000000013\n",
      "1.0000032999999977\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "1.0000084000000031\n",
      "1.0000015999999965\n",
      "0.9999993000000005\n",
      "1.0000023000000056\n",
      "1.0000068000000013\n",
      "1.0000032999999977\n",
      "1.0000026999999998\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "1.0000084000000031\n",
      "1.0000015999999965\n",
      "0.9999993000000005\n",
      "1.0000023000000056\n",
      "1.0000068000000013\n",
      "1.0000032999999977\n",
      "1.0000026999999998\n",
      "1.0000042\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "1.0000084000000031\n",
      "1.0000015999999965\n",
      "0.9999993000000005\n",
      "1.0000023000000056\n",
      "1.0000068000000013\n",
      "1.0000032999999977\n",
      "1.0000026999999998\n",
      "1.0000042\n",
      "1.0000036000000032\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "1.0000084000000031\n",
      "1.0000015999999965\n",
      "0.9999993000000005\n",
      "1.0000023000000056\n",
      "1.0000068000000013\n",
      "1.0000032999999977\n",
      "1.0000026999999998\n",
      "1.0000042\n",
      "1.0000036000000032\n",
      "1.0000031999999976\n",
      "0.9999989999999949\n",
      "1.0000040999999942\n",
      "0.9999961999999971\n",
      "1.000007800000004\n",
      "1.000009800000004\n",
      "1.0000102000000013\n",
      "1.0000101999999986\n",
      "1.0000164999999963\n",
      "1.0000095000000015\n",
      "1.0000066999999975\n",
      "1.0000094000000008\n",
      "1.0000053000000002\n",
      "0.9999999999999999\n",
      "1.0000002999999984\n",
      "0.9999990000000045\n",
      "0.9999991000000047\n",
      "1.0000049999999987\n",
      "1.0000108999999981\n",
      "1.0000063999999993\n",
      "1.0000084000000031\n",
      "1.0000015999999965\n",
      "0.9999993000000005\n",
      "1.0000023000000056\n",
      "1.0000068000000013\n",
      "1.0000032999999977\n",
      "1.0000026999999998\n",
      "1.0000042\n",
      "1.0000036000000032\n",
      "1.0000031999999976\n",
      "1.0000035000000014\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = penguin_dataset[penguin_dataset.columns[:-1]].to_numpy()\n",
    "y = penguin_dataset[\"species\"]\n",
    "\n",
    "indexes = []\n",
    "sklearn_acc = []\n",
    "model_acc = []\n",
    "for index in range(1,31):\n",
    "    indexes.append(index)\n",
    "\n",
    "    ab = AdaBoostClassifier(algorithm=\"SAMME\", n_estimators=index).fit(X, y)\n",
    "    sklearn_acc.append(ab.score(X,y))\n",
    "\n",
    "    my_ab = adaboost(penguin_dataset, \"species\", index)\n",
    "    model_acc.append(adaboost_score(my_ab, X, y))\n",
    "\n",
    "# saved_model_acc = [0.7867867867867868, 0.7867867867867868, 0.963963963963964, 0.8138138138138138, 0.972972972972973, 0.9669669669669669, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976]\n",
    "plt.plot(indexes, sklearn_acc, label = \"ssklearn_acc\")\n",
    "plt.plot(indexes, model_acc, label = \"model_acc\")\n",
    "plt.legend()\n",
    "plt.title('comparison')\n",
    "plt.xlabel('index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7867867867867868, 0.7867867867867868, 0.963963963963964, 0.8138138138138138, 0.972972972972973, 0.9669669669669669, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976]\n"
     ]
    }
   ],
   "source": [
    "saved_model_acc = [0.7867867867867868, 0.7867867867867868, 0.963963963963964, 0.8138138138138138, 0.972972972972973, 0.9669669669669669, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976, 0.972972972972973, 0.975975975975976]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise grading\n",
    "\n",
    "| Section | Exercise | Points |\n",
    "| --- | --- | --- |\n",
    "| I |\t1 | \t0.1 |\n",
    "| I |\t2 | \t0.1 |\n",
    "| I |\t3 | \t0.25 |\n",
    "| I |\t4 | \t0.25 |\n",
    "| I |\t5 | \t0.15 |\n",
    "| I |\t6 | \t0.65 |\n",
    "| I |\t7 | \t0.1 |\n",
    "| I |\t8 | \t0.25 |\n",
    "| I |\t9 | \t0.15 |\n",
    "| I |\t10 | \t0.1 |\n",
    "| I |\t11 | \t0.1 |\n",
    "| I |\t12 | \t0.1 |\n",
    "| I |\t13 | \t0.15 |\n",
    "| I |\t14 | \t0.65 |\n",
    "| I |\t15 | \t0.1 |\n",
    "| I |\t16 | \t0.2 |\n",
    "| I |\t17 | \t0.1 |\n",
    "|II | \t1 | \t0.2 |\n",
    "|II | \t2 | \t0.1 |\n",
    "|II | \t3 | \t0.15 |\n",
    "|II | \t4 | \t0.2 |\n",
    "|II | \t5 | \t0.72 |\n",
    "|II | \t6 | \t0.1 |\n",
    "|II | \t7 | \t0.28 |\n",
    "|II | \t8 | \t0.1 |\n",
    "|II | \t9 | \t0.15 |\n",
    "|III |  1 | 0.1 |\n",
    "|III |  2 | 0.2 |\n",
    "|III |  3 | 0.3 |\n",
    "|III |  4 | 0.15 |\n",
    "|III |  5 | 0.15 |\n",
    "|III |  6 | 0.3 |\n",
    "|III |  7 | 0.5 |\n",
    "|III |  8 | 0.4 |\n",
    "|III |  9 | 0.1 |\n",
    "|III |  10 | 0.5 |\n",
    "|III |  11 | 0.2 |\n",
    "|III |  12 | 1.1 |\n",
    "|III |  13 | 0.2 |\n",
    "|III |  14 | 0.1 |\n",
    "|III |  15 | 0.2 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59c2697042bf23f669049604bc51c7fdaddd84c684e590656d4c80c751d79f30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
